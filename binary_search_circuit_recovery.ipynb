{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff754",
   "metadata": {},
   "source": [
    "# Symbolic Regression Tutorial: Binary Search Circuit Recovery\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to recover an arithmetic circuit for binary search using symbolic regression. We'll show how the binary search algorithm can be \"unrolled\" into polynomial constraints and use pySR to recover each component.\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "Our binary search circuit operates on a sorted array $A$ and target value $t$. For each iteration $i$, the circuit consists of:\n",
    "\n",
    "### 1. State Variables\n",
    "- Left endpoint: $L_i$ \n",
    "- Right endpoint: $R_i$\n",
    "\n",
    "### 2. Midpoint Computation\n",
    "The midpoint $m_i$ is computed with the constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "(2m_i - L_i - R_i)^2 = 0\n",
    "\\end{equation}\n",
    "\n",
    "In the ideal case, this simplifies to:\n",
    "\n",
    "\\begin{equation}\n",
    "m_i = \\frac{L_i + R_i}{2}\n",
    "\\end{equation}\n",
    "\n",
    "### 3. Branch Indicators\n",
    "We have three Boolean indicators:\n",
    "- Equality: $\\delta_i^=$\n",
    "- Less than: $\\delta_i^<$\n",
    "- Greater than: $\\delta_i^>$\n",
    "\n",
    "These satisfy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_i^= + \\delta_i^< + \\delta_i^> = 1\n",
    "\\end{equation}\n",
    "\n",
    "### 4. State Updates\n",
    "When the less-than branch is active ($\\delta_i^< = 1$), we update:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{i+1} = \\delta_i^< \\cdot (m_i + 1) + (1 - \\delta_i^<)L_i\n",
    "\\end{equation}\n",
    "\n",
    "### 5. Output Computation\n",
    "The final output is:\n",
    "\n",
    "\\begin{equation}\n",
    "o = \\sum_{i=0}^{T-1} \\delta_i^= \\cdot m_i + \\left(1 - \\sum_{i=0}^{T-1}\\delta_i^=\\right)(-1)\n",
    "\\end{equation}\n",
    "\n",
    "where $T = \\lceil \\log_2(n) \\rceil + 1$ is the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9cb8f",
   "metadata": {},
   "source": [
    "We define the number of iterations $T$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "T = \\left\\lceil \\log_2(n) \\right\\rceil + 1\n",
    "\\end{equation}\n",
    "\n",
    "We simulate the standard binary search. Once the target is found in some iteration, we \"freeze\" the state for the remaining iterations (and we set the branch indicator for that iteration to 1, and for later iterations to 0). In the output constraint we define:\n",
    "\n",
    "\\begin{equation}\n",
    "o = \\sum_{i=0}^{T-1} \\delta_i^= m_i + \\left(1-\\sum_{i=0}^{T-1}\\delta_i^=\\right)(-1)\n",
    "\\end{equation}\n",
    "\n",
    "Thus, if the target is found in exactly one iteration, the sum is just that $m_i$ (the index where the target appears); if not found (so all $\\delta_i^= = 0$), then $o = -1$.\n",
    "\n",
    "We then define a function to check all the polynomial constraints (midpoint, branch sum, comparison, and state update) using SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20981fe8-165c-4eb1-a5d9-cb079d12da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [code]\n",
    "import math\n",
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variables for T iterations.\n",
    "def define_symbols(T):\n",
    "    L_syms = sp.symbols('L0:'+str(T), real=True)\n",
    "    R_syms = sp.symbols('R0:'+str(T), real=True)\n",
    "    m_syms = sp.symbols('m0:'+str(T), real=True)\n",
    "    delta_eq_syms = sp.symbols('delta0_eq:'+str(T)+'_eq', real=True)\n",
    "    delta_lt_syms = sp.symbols('delta0_lt:'+str(T)+'_lt', real=True)\n",
    "    delta_gt_syms = sp.symbols('delta0_gt:'+str(T)+'_gt', real=True)\n",
    "    return L_syms, R_syms, m_syms, delta_eq_syms, delta_lt_syms, delta_gt_syms\n",
    "\n",
    "# simulate_binary_search now works for any sorted A and target.\n",
    "def simulate_binary_search(A, target):\n",
    "    n = len(A)\n",
    "    # Use T = ceil(log2(n)) + 1 iterations\n",
    "    T = math.ceil(math.log(n, 2)) + 1\n",
    "    L_syms, R_syms, m_syms, delta_eq_syms, delta_lt_syms, delta_gt_syms = define_symbols(T)\n",
    "    \n",
    "    assign = {}\n",
    "    found = False   # flag: whether we've found the target already.\n",
    "    found_iter = None  # which iteration first found the target.\n",
    "    \n",
    "    # Initialize L0 and R0:\n",
    "    assign[L_syms[0]] = 0\n",
    "    assign[R_syms[0]] = n - 1\n",
    "    \n",
    "    for i in range(T):\n",
    "        # If the state has already been \"frozen\" because we found the target,\n",
    "        # then for later iterations we simply copy the previous state and set branch indicators to 0.\n",
    "        if found:\n",
    "            assign[m_syms[i]] = assign[m_syms[i-1]]\n",
    "            assign[L_syms[i]] = assign[L_syms[i-1]]\n",
    "            assign[R_syms[i]] = assign[R_syms[i-1]]\n",
    "            assign[delta_eq_syms[i]] = 0\n",
    "            assign[delta_lt_syms[i]] = 0\n",
    "            assign[delta_gt_syms[i]] = 0\n",
    "            continue\n",
    "        \n",
    "        L_val = assign[L_syms[i]]\n",
    "        R_val = assign[R_syms[i]]\n",
    "        # If the interval is empty (L > R), then no valid search remains.\n",
    "        # In that case, we simply set m to an arbitrary value (say, L_val) and no branch indicator.\n",
    "        if L_val > R_val:\n",
    "            assign[m_syms[i]] = L_val\n",
    "            assign[delta_eq_syms[i]] = 0\n",
    "            assign[delta_lt_syms[i]] = 0\n",
    "            assign[delta_gt_syms[i]] = 0\n",
    "        else:\n",
    "            m_val = (L_val + R_val) // 2  # floor division\n",
    "            assign[m_syms[i]] = m_val\n",
    "            # Decide the branch based on A[m_val]\n",
    "            if A[m_val] == target:\n",
    "                assign[delta_eq_syms[i]] = 1\n",
    "                assign[delta_lt_syms[i]] = 0\n",
    "                assign[delta_gt_syms[i]] = 0\n",
    "                found = True\n",
    "                found_iter = i\n",
    "            elif A[m_val] < target:\n",
    "                assign[delta_eq_syms[i]] = 0\n",
    "                assign[delta_lt_syms[i]] = 1\n",
    "                assign[delta_gt_syms[i]] = 0\n",
    "            else:  # A[m_val] > target\n",
    "                assign[delta_eq_syms[i]] = 0\n",
    "                assign[delta_lt_syms[i]] = 0\n",
    "                assign[delta_gt_syms[i]] = 1\n",
    "        \n",
    "        # State update for next iteration, if not the last iteration.\n",
    "        if i < T - 1:\n",
    "            # If we already found the target, then copy state.\n",
    "            if found:\n",
    "                assign[L_syms[i+1]] = assign[L_syms[i]]\n",
    "                assign[R_syms[i+1]] = assign[R_syms[i]]\n",
    "            else:\n",
    "                # For left update: if delta_lt is 1, set L_{i+1} = m_i + 1; otherwise, L remains.\n",
    "                L_next = assign[delta_lt_syms[i]] * (assign[m_syms[i]] + 1) + (1 - assign[delta_lt_syms[i]]) * assign[L_syms[i]]\n",
    "                # For right update: if delta_gt is 1, set R_{i+1} = m_i - 1; otherwise, R remains.\n",
    "                R_next = assign[delta_gt_syms[i]] * (assign[m_syms[i]] - 1) + (1 - assign[delta_gt_syms[i]]) * assign[R_syms[i]]\n",
    "                assign[L_syms[i+1]] = L_next\n",
    "                assign[R_syms[i+1]] = R_next\n",
    "    # Compute the output o.\n",
    "    o_val = 0\n",
    "    sum_delta = 0\n",
    "    for i in range(T):\n",
    "        o_val += assign[delta_eq_syms[i]] * assign[m_syms[i]]\n",
    "        sum_delta += assign[delta_eq_syms[i]]\n",
    "    # If target never found, output -1.\n",
    "    o_val = o_val + (1 - sum_delta) * (-1)\n",
    "    assign['o'] = o_val\n",
    "    vars_dict = {\n",
    "        'L': L_syms,\n",
    "        'R': R_syms,\n",
    "        'm': m_syms,\n",
    "        'delta_eq': delta_eq_syms,\n",
    "        'delta_lt': delta_lt_syms,\n",
    "        'delta_gt': delta_gt_syms\n",
    "    }\n",
    "    return assign, vars_dict\n",
    "\n",
    "# The same check_constraints function as before.\n",
    "def check_constraints(assign, vars_dict, A, target):\n",
    "    L_syms = vars_dict['L']\n",
    "    R_syms = vars_dict['R']\n",
    "    m_syms = vars_dict['m']\n",
    "    delta_eq_syms = vars_dict['delta_eq']\n",
    "    delta_lt_syms = vars_dict['delta_lt']\n",
    "    delta_gt_syms = vars_dict['delta_gt']\n",
    "    \n",
    "    T = len(m_syms)\n",
    "    print(\"Checking constraints for each iteration:\")\n",
    "    for i in range(T):\n",
    "        print(f\"\\nIteration {i}:\")\n",
    "        # Midpoint constraint: 2*m_i - L_i - R_i = 0.\n",
    "        eq_mid = sp.simplify(2*m_syms[i] - L_syms[i] - R_syms[i])\n",
    "        val_mid = eq_mid.subs(assign)\n",
    "        print(f\"  Midpoint: 2*m_{i} - L_{i} - R_{i} = {val_mid}\")\n",
    "        \n",
    "        # Branch sum constraint: delta_eq + delta_lt + delta_gt - 1 = 0.\n",
    "        eq_branch = sp.simplify(delta_eq_syms[i] + delta_lt_syms[i] + delta_gt_syms[i] - 1)\n",
    "        val_branch = eq_branch.subs(assign)\n",
    "        print(f\"  Branch sum: delta_eq_{i} + delta_lt_{i} + delta_gt_{i} - 1 = {val_branch}\")\n",
    "        \n",
    "        # Comparison constraints:\n",
    "        m_val = assign[m_syms[i]]\n",
    "        diff = A[m_val] - target\n",
    "        eq_comp_eq = sp.simplify(delta_eq_syms[i] * diff)\n",
    "        val_comp_eq = eq_comp_eq.subs(assign)\n",
    "        print(f\"  Equality compare: delta_eq_{i}*(A[m_{i}]-target) = {val_comp_eq}\")\n",
    "        eq_comp_lt = sp.simplify(delta_lt_syms[i] * (target - A[m_val]))\n",
    "        val_comp_lt = eq_comp_lt.subs(assign)\n",
    "        print(f\"  Less-than compare: delta_lt_{i}*(target-A[m_{i}]) = {val_comp_lt}\")\n",
    "        eq_comp_gt = sp.simplify(delta_gt_syms[i] * (A[m_val]-target))\n",
    "        val_comp_gt = eq_comp_gt.subs(assign)\n",
    "        print(f\"  Greater-than compare: delta_gt_{i}*(A[m_{i}]-target) = {val_comp_gt}\")\n",
    "        \n",
    "        # State update constraints (if not the last iteration)\n",
    "        if i < T - 1:\n",
    "            expr_L_next = sp.simplify(delta_lt_syms[i]*(m_syms[i] + 1) + (1 - delta_lt_syms[i])*L_syms[i])\n",
    "            eq_update_L = sp.simplify(L_syms[i+1] - expr_L_next)\n",
    "            val_update_L = eq_update_L.subs(assign)\n",
    "            print(f\"  L update: L_{i+1} - [delta_lt_{i}*(m_{i}+1) + (1-delta_lt_{i})*L_{i}] = {val_update_L}\")\n",
    "            \n",
    "            expr_R_next = sp.simplify(delta_gt_syms[i]*(m_syms[i] - 1) + (1 - delta_gt_syms[i])*R_syms[i])\n",
    "            eq_update_R = sp.simplify(R_syms[i+1] - expr_R_next)\n",
    "            val_update_R = eq_update_R.subs(assign)\n",
    "            print(f\"  R update: R_{i+1} - [delta_gt_{i}*(m_{i}-1) + (1-delta_gt_{i})*R_{i}] = {val_update_R}\")\n",
    "    # Compute and print the output.\n",
    "    o_expr = sum(delta_eq_syms[i] * m_syms[i] for i in range(T)) + (1 - sum(delta_eq_syms[i] for i in range(T)))*(-1)\n",
    "    o_computed = sp.simplify(o_expr).subs(assign)\n",
    "    print(f\"\\nComputed output o = {o_computed}\")\n",
    "    return o_computed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b92e2-2e30-4a60-9f67-963370832b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "target1 = 12\n",
    "assign1, vars_dict1 = simulate_binary_search(A1, target1)\n",
    "print(\"Example 1: Target is in A\")\n",
    "print(\"Witness assignment:\")\n",
    "for key, val in assign1.items():\n",
    "    print(f\"  {key} = {val}\")\n",
    "print(\"\\nConstraint check:\")\n",
    "o1 = check_constraints(assign1, vars_dict1, A1, target1)\n",
    "print(\"\\nFinal output (should be index of 10):\", o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dc6f9-3ddd-490d-b990-2fb7389e37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "target2 = 9\n",
    "assign2, vars_dict2 = simulate_binary_search(A2, target2)\n",
    "print(\"\\nExample 2: Target is NOT in A\")\n",
    "print(\"Witness assignment:\")\n",
    "for key, val in assign2.items():\n",
    "    print(f\"  {key} = {val}\")\n",
    "print(\"\\nConstraint check:\")\n",
    "o2 = check_constraints(assign2, vars_dict2, A2, target2)\n",
    "print(\"\\nFinal output (should be -1):\", o2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd624bc5",
   "metadata": {},
   "source": [
    "# Binary Search Circuit Recovery via Symbolic Regression\n",
    "\n",
    "In this tutorial we explain step by step how the binary search algorithm can be \"unrolled\" into an arithmetic circuit—that is, a set of polynomial constraints. We then illustrate how one might use a symbolic regression tool (pySR) to recover each small module (or \"gate\") of the circuit.\n",
    "\n",
    "Our binary search circuit (for a given sorted array $A$ and target $t$) has several layers.\n",
    "\n",
    "For each iteration $i$, we have:\n",
    "\n",
    "1. **State variables:** $L_i$ (left endpoint), $R_i$ (right endpoint)\n",
    "\n",
    "2. **Computation node:** $m_i$, the midpoint, with the constraint:\n",
    "   \\begin{equation}\n",
    "   (2m_i - L_i - R_i)^2 = 0\n",
    "   \\end{equation}\n",
    "   (In the ideal circuit, $m_i = \\frac{L_i + R_i}{2}$)\n",
    "\n",
    "3. **Decision nodes:** Boolean branch indicators $\\delta_i^=, \\delta_i^<, \\delta_i^>$ satisfying:\n",
    "   \\begin{equation}\n",
    "   \\delta_i^= + \\delta_i^< + \\delta_i^> = 1\n",
    "   \\end{equation}\n",
    "   They \"select\" which branch is taken based on comparing $A[m_i]$ and $t$\n",
    "\n",
    "4. **State update nodes:** For example, if the less-than branch is active ($\\delta_i^< = 1$), the left endpoint is updated as:\n",
    "   \\begin{equation}\n",
    "   L_{i+1} = \\delta_i^< \\cdot (m_i + 1) + (1 - \\delta_i^<)L_i\n",
    "   \\end{equation}\n",
    "\n",
    "5. **Output node:** Finally, the output is defined as:\n",
    "   \\begin{equation}\n",
    "   o = \\left(\\sum_{i=0}^{T-1} \\delta_i^= \\cdot m_i\\right) + \\left(1 - \\sum_{i=0}^{T-1} \\delta_i^=\\right)(-1)\n",
    "   \\end{equation}\n",
    "   so that if an equality branch is taken, the corresponding $m_i$ is output; otherwise $o = -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc11d6a",
   "metadata": {},
   "source": [
    "## Key Components to Recover\n",
    "\n",
    "We'll focus on recovering three essential modules:\n",
    "\n",
    "1. **Midpoint Function**\n",
    "   \\begin{equation}\n",
    "   m = \\frac{L + R}{2}\n",
    "   \\end{equation}\n",
    "\n",
    "2. **Left State Update**\n",
    "   \\begin{equation}\n",
    "   L_\\text{next} = \\delta_\\text{lt}(m+1) + (1-\\delta_\\text{lt})L\n",
    "   \\end{equation}\n",
    "\n",
    "3. **Output Function**\n",
    "   \\begin{equation}\n",
    "   o = \\delta_\\text{eq} \\cdot m + (1-\\delta_\\text{eq})(-1)\n",
    "   \\end{equation}\n",
    "\n",
    "Each module will be learned using symbolic regression on simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6913e9e",
   "metadata": {},
   "source": [
    "## Recovering the Midpoint Function\n",
    "\n",
    "In a correct binary search the midpoint is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "m = \\frac{L + R}{2}\n",
    "\\end{equation}\n",
    "\n",
    "We generate data by sampling random values for $L$ and $R$ (with $L < R$) and then setting $m$ to $\\frac{L+R}{2}$. Our goal is to see if pySR can recover this linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d955f4-db45-4de4-bb54-792ecf25f283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [code]\n",
    "import numpy as np\n",
    "from pysr import PySRRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %% [code]\n",
    "# Number of samples for training\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate random L and R values\n",
    "L_vals = np.random.uniform(0, 5, num_samples)\n",
    "# Ensure R > L by choosing R uniformly in (L+1, L+6)\n",
    "R_vals = np.array([np.random.uniform(L + 1, L + 6) for L in L_vals])\n",
    "m_vals = (L_vals + R_vals) / 2\n",
    "\n",
    "# Training data: inputs are [L, R] and output is m.\n",
    "X_mid = np.column_stack([L_vals, R_vals])\n",
    "y_mid = m_vals\n",
    "\n",
    "# Setup pySR to recover the midpoint function.\n",
    "model_mid = PySRRegressor(\n",
    "    niterations=1000,\n",
    "    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "    unary_operators=[],\n",
    "    model_selection=\"best\",\n",
    "    maxsize=7,\n",
    "    loss=\"loss(x, y) = (x - y)^2\",\n",
    "    verbosity=1,\n",
    ")\n",
    "model_mid.fit(X_mid, y_mid)\n",
    "print(\"Recovered midpoint function:\")\n",
    "print(model_mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d86b40",
   "metadata": {},
   "source": [
    "## Recovering the Left State Update Function\n",
    "\n",
    "Next, we recover the function that updates the left endpoint. In our arithmetic circuit, if we are in the less-than branch, the left endpoint is updated as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\text{next}} = \\delta_{\\text{lt}} \\cdot (m + 1) + (1 - \\delta_{\\text{lt}}) \\cdot L\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\delta_{\\text{lt}}$ is a Boolean variable (0 or 1). We simulate training data by randomly generating values for $L$ and $m$, and a random binary value for $\\delta_{\\text{lt}}$, then computing $L_{\\text{next}}$ accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b1e36-ad6b-4f67-b2a2-fa86d01df27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [code]\n",
    "num_samples = 1000\n",
    "L_vals = np.random.uniform(0, 5, num_samples)\n",
    "m_vals = np.random.uniform(5, 10, num_samples)\n",
    "# δ_lt is either 0 or 1\n",
    "delta_lt = np.random.randint(0, 2, num_samples)\n",
    "\n",
    "L_next = delta_lt * (m_vals + 1) + (1 - delta_lt) * L_vals\n",
    "\n",
    "# Training data: inputs are [L, m, δ_lt] and output is L_next.\n",
    "X_state = np.column_stack([L_vals, m_vals, delta_lt])\n",
    "y_state = L_next\n",
    "\n",
    "model_state = PySRRegressor(\n",
    "    niterations=1000,\n",
    "    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "    unary_operators=[],\n",
    "    model_selection=\"best\",\n",
    "    maxsize=10,\n",
    "    loss=\"loss(x, y) = (x - y)^2\",\n",
    "    verbosity=1,\n",
    ")\n",
    "model_state.fit(X_state, y_state)\n",
    "print(\"\\nRecovered left update function:\")\n",
    "print(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23c794",
   "metadata": {},
   "source": [
    "## Recovering the Final Output Function\n",
    "\n",
    "Finally, we try to recover the output function. Suppose that at the relevant layer we have a branch indicator $\\delta_{\\text{eq}}$ for the equality branch and a midpoint $m$. Our output is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "o = \\delta_{\\text{eq}} \\cdot m + (1 - \\delta_{\\text{eq}}) \\cdot (-1)\n",
    "\\end{equation}\n",
    "\n",
    "Thus, if $\\delta_{\\text{eq}} = 1$ the output is $m$; if $\\delta_{\\text{eq}} = 0$ the output is $-1$.\n",
    "\n",
    "We simulate data by choosing random $m$ values and random binary values for $\\delta_{\\text{eq}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8c055-f88b-4960-9b54-bec681107bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [code]\n",
    "num_samples = 1000\n",
    "m_vals = np.random.uniform(0, 10, num_samples)\n",
    "delta_eq = np.random.randint(0, 2, num_samples)\n",
    "\n",
    "o_vals = delta_eq * m_vals + (1 - delta_eq) * (-1)\n",
    "\n",
    "# Training data: inputs are [m, δ_eq] and output is o.\n",
    "X_out = np.column_stack([m_vals, delta_eq])\n",
    "y_out = o_vals\n",
    "\n",
    "model_out = PySRRegressor(\n",
    "    niterations=1000,\n",
    "    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "    unary_operators=[],\n",
    "    model_selection=\"best\",\n",
    "    maxsize=10,\n",
    "    loss=\"loss(x, y) = (x - y)^2\",\n",
    "    verbosity=1,\n",
    ")\n",
    "model_out.fit(X_out, y_out)\n",
    "print(\"\\nRecovered output function:\")\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5bce1",
   "metadata": {},
   "source": [
    "## Custom Operators and Recovered Module Functions\n",
    "\n",
    "We define our custom operator and the functions for the modules as recovered previously:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{sel}(x,y,z) &= xy + (1-x)z \\\\\n",
    "\\text{mid}(L,R) &= \\frac{L + R}{2} \\\\\n",
    "\\text{left\\_update}(L,m,\\delta_{\\text{lt}}) &= \\text{sel}(\\delta_{\\text{lt}}, m+1, L) \\\\\n",
    "\\text{output\\_module}(m,\\delta_{\\text{eq}}) &= \\text{sel}(\\delta_{\\text{eq}}, m, -1)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sel(x, y, z):\n",
    "    \"\"\"Custom operator: if x is 1, returns y; if x is 0, returns z.\"\"\"\n",
    "    return x * y + (1 - x) * z\n",
    "\n",
    "def mid(L, R):\n",
    "    \"\"\"Midpoint function: returns (L + R)/2.\"\"\"\n",
    "    return (L + R) / 2\n",
    "\n",
    "def left_update(L, m, delta_lt):\n",
    "    \"\"\"Left state update: L_next = sel(delta_lt, m+1, L).\"\"\"\n",
    "    return sel(delta_lt, m + 1, L)\n",
    "\n",
    "def output_module(m, delta_eq):\n",
    "    \"\"\"Output module: o = sel(delta_eq, m, -1).\"\"\"\n",
    "    return sel(delta_eq, m, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454c70f",
   "metadata": {},
   "source": [
    "## Full Circuit Composition\n",
    "\n",
    "We'll compose the recovered modules using a custom selection operator:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sel}(x,y,z) = xy + (1-x)z\n",
    "\\end{equation}\n",
    "\n",
    "For our example array $A = [2, 4, 6, 8, 10, 12, 14, 16]$, the ideal mapping is:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) = \\begin{cases}\n",
    "\\frac{t-2}{2} & \\text{if } t \\in A \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "We'll use symbolic regression to recover an approximation of this piecewise function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO Understand this section a bit more.... does it generalize to other inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc6010",
   "metadata": {},
   "source": [
    "## Full Circuit Simulation: Generating Input–Output Pairs\n",
    "\n",
    "We now define a function that simulates the full binary search circuit. The array $A$ is hard-wired, and the only public input is the target $t$.\n",
    "\n",
    "The circuit is unrolled for $T$ iterations (with $T = \\left\\lceil \\log_2(n) \\right\\rceil + 1$). At each iteration, the circuit computes the midpoint and then decides the branch:\n",
    "\n",
    "- If $A[m] = t$, we set $\\delta_{\\text{eq}} = 1$ and freeze the state\n",
    "- If $A[m] < t$, we set $\\delta_{\\text{lt}} = 1$\n",
    "- If $A[m] > t$, we set $\\delta_{\\text{gt}} = 1$\n",
    "\n",
    "The left state is updated using $\\text{left\\_update}$. Finally, the output is computed using $\\text{output\\_module}$ at the iteration where $\\delta_{\\text{eq}} = 1$.\n",
    "\n",
    "We then generate $(t, o)$ pairs for targets in a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992ece2-ed1d-4091-8091-6f4972dd1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [code]\n",
    "import math\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "def simulate_full_circuit(A, t):\n",
    "    \"\"\"\n",
    "    Simulate a full binary search circuit for a fixed sorted array A and target t.\n",
    "    Returns the final output o (index if t is in A, -1 otherwise).\n",
    "    \"\"\"\n",
    "    n = len(A)\n",
    "    T = math.ceil(math.log(n, 2)) + 1  # Unroll for T iterations.\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    found = False\n",
    "    output = -1\n",
    "    for i in range(T):\n",
    "        # Compute midpoint (using floor for index).\n",
    "        m_val = math.floor(mid(L + R))\n",
    "        # Decide branch if within array bounds.\n",
    "        if not found and 0 <= m_val < n:\n",
    "            if A[m_val] == t:\n",
    "                delta_eq = 1\n",
    "                delta_lt = 0\n",
    "                delta_gt = 0\n",
    "                found = True\n",
    "            elif A[m_val] < t:\n",
    "                delta_eq = 0\n",
    "                delta_lt = 1\n",
    "                delta_gt = 0\n",
    "            else:\n",
    "                delta_eq = 0\n",
    "                delta_lt = 0\n",
    "                delta_gt = 1\n",
    "        else:\n",
    "            delta_eq = 0\n",
    "            delta_lt = 0\n",
    "            delta_gt = 0\n",
    "        # Update state: if target not found, update using our modules.\n",
    "        if i < T - 1:\n",
    "            if not found:\n",
    "                L = left_update(L, m_val, delta_lt)\n",
    "                # For simplicity, update R using a similar rule (here we use a direct update)\n",
    "                R = sel(delta_gt, m_val - 1, R)\n",
    "            # Once found, state remains frozen.\n",
    "    # Final output: if found, use output_module from the iteration where it was found.\n",
    "    if found:\n",
    "        output = output_module(m_val, 1)  # since δ_eq was set to 1.\n",
    "    else:\n",
    "        output = -1\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e34c2b",
   "metadata": {},
   "source": [
    "## Symbolic Regression to Recover the Full Mapping\n",
    "\n",
    "Our goal is to use pySR to recover a symbolic expression for the overall mapping $f(t) = o$. The ideal mapping for our toy array is piecewise:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) = \\begin{cases}\n",
    "\\frac{t - 2}{2} & \\text{if } t \\in A \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "We now let pySR search for an expression in terms of $t$ (using our custom operators as candidate building blocks) that fits the data.\n",
    "\n",
    "We supply our custom operator dictionary, which includes:\n",
    "- $\\text{sel}$\n",
    "- $\\text{mid}$\n",
    "- $\\text{left\\_update}$\n",
    "- $\\text{output\\_module}$\n",
    "\n",
    "**Note:** Recovering a piecewise function exactly is challenging; pySR might find an expression that approximates the behavior over our small target range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363ad1a-2eb0-4e84-8aad-6150ad12907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "\n",
    "# Generate data for a range of target values.\n",
    "A_fixed = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "target_values = np.arange(1, 18)  # t = 1,2,...,17\n",
    "outputs = [simulate_full_circuit(A_fixed, t) for t in target_values]\n",
    "\n",
    "data = np.column_stack([target_values, outputs])\n",
    "print(\"Full circuit simulation data (target, output):\")\n",
    "print(data)\n",
    "X_full = target_values.reshape(-1, 1)\n",
    "y_full = np.array(outputs)\n",
    "\n",
    "custom_ops_full = {\n",
    "    \"sel\": sel,\n",
    "    \"mid\": mid,\n",
    "    \"left_update\": left_update,\n",
    "    \"output_module\": output_module\n",
    "}\n",
    "\n",
    "model_full = PySRRegressor(\n",
    "    niterations=5000,\n",
    "    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "    unary_operators=[],\n",
    "    extra_sympy_mappings=custom_ops_full,\n",
    "    model_selection=\"best\",\n",
    "    maxsize=12,\n",
    "    loss=\"loss(x, y) = (x - y)^2\",\n",
    "    verbosity=1,\n",
    ")\n",
    "model_full.fit(X_full, y_full)\n",
    "print(\"\\nRecovered full mapping from target t to output o:\")\n",
    "print(model_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87be2c0",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this tutorial we demonstrated an end-to-end approach for recovering the entire arithmetic circuit that encodes the binary search algorithm:\n",
    "\n",
    "1. We assumed that symbolic regression had already recovered the building blocks (modules) such as:\n",
    "   \n",
    "   \\begin{align*}\n",
    "   \\text{mid}(L, R) &= \\frac{L + R}{2} \\\\\n",
    "   \\text{left\\_update}(L, m, \\delta_{\\text{lt}}) &= \\text{sel}(\\delta_{\\text{lt}}, m+1, L) \\\\\n",
    "   \\text{output\\_module}(m, \\delta_{\\text{eq}}) &= \\text{sel}(\\delta_{\\text{eq}}, m, -1)\n",
    "   \\end{align*}\n",
    "   \n",
    "   using our custom operator $\\text{sel}$.\n",
    "\n",
    "2. We composed these modules into a full circuit by simulating an unrolled binary search run (for $T$ iterations) on a fixed sorted array $A$ and a public target $t$.\n",
    "\n",
    "3. We generated $(t, o)$ pairs from many runs and then applied pySR (with our custom operators) to recover a symbolic expression for the overall mapping from target to output.\n",
    "\n",
    "The ideal mapping for our evenly spaced array $A = [2, 4, 6, 8, 10, 12, 14, 16]$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) = \\text{sel}(\\text{inA}(t), \\frac{t - 2}{2}, -1)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{inA}(t)$ is an indicator function that equals 1 when $t \\in A$ and 0 otherwise.\n",
    "\n",
    "Although recovering the exact piecewise function is challenging, the recovered expression (or candidate expressions) should reflect the structure of the full circuit and the composition of the modules.\n",
    "\n",
    "This demonstrates that symbolic regression can be used directly (via composition and custom operators) to recover an entire arithmetic circuit from input–output pairs.\n",
    "\n",
    "Happy exploring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27d9e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbo",
   "language": "python",
   "name": "symbo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
